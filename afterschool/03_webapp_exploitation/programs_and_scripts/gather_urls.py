import requests
from bs4 import BeautifulSoup, Comment
from urllib.parse import urlparse, urljoin

def collect_links(url):
    # Fetch the webpage
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch:", url)
        return []
    
    # Parse the webpage content
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Collect all links found in the header section of the webpage
    header_links = set()
    header = soup.find('header')
    if header:
        for link in header.find_all('a', href=True):
            header_links.add(urljoin(url, link['href']))
    
    # Collect all links found in the webpage
    all_links = set()
    for link in soup.find_all(['a', 'link'], href=True):
        all_links.add(urljoin(url, link['href']))
    
    # Collect links from href and src attributes in script tags
    for script in soup.find_all('script'):
        if script.get('src'):
            all_links.add(urljoin(url, script['src']))
    
    # Collect links from src attributes in img tags
    for img in soup.find_all('img'):
        if img.get('src'):
            all_links.add(urljoin(url, img['src']))
    
    # Collect links from href attributes in comments
    for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
        comment_soup = BeautifulSoup(comment, 'html.parser')
        for link in comment_soup.find_all('a', href=True):
            all_links.add(urljoin(url, link['href']))
    
    return list(all_links)

def write_urls_to_file(urls, file_path):
    with open(file_path, 'w') as file:
        for url in urls:
            file.write(url + '\n')

# Example usage:
starting_url = 'https://jupiter.challenges.picoctf.org/problem/9670/'
collected_links = collect_links(starting_url)
collected_links.append(starting_url)  # Append the starting_url to the collected links
write_urls_to_file(collected_links, 'urls.txt')
