import requests
from bs4 import BeautifulSoup, Comment
from urllib.parse import urlparse, urljoin

def collect_links(url):
    # Fetch the webpage
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch:", url)
        return []
    
    # Parse the webpage content
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Collect all links found in the header section of the webpage
    header_links = set()
    header = soup.find('header')
    if header:
        for link in header.find_all('a', href=True):
            header_links.add(urljoin(url, link['href']))
    
    # Collect all links found in the webpage
    all_links = set()
    for link in soup.find_all(['a', 'link'], href=True):
        all_links.add(urljoin(url, link['href']))
    
    # Collect links from href and src attributes in script tags
    for script in soup.find_all('script'):
        if script.get('src'):
            all_links.add(urljoin(url, script['src']))
    
    # Collect links from src attributes in img tags
    for img in soup.find_all('img'):
        if img.get('src'):
            all_links.add(urljoin(url, img['src']))
    
    # Collect links from href attributes in comments
    for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
        comment_soup = BeautifulSoup(comment, 'html.parser')
        for link in comment_soup.find_all('a', href=True):
            all_links.add(urljoin(url, link['href']))
    
    # Print all collected links
    print("All links found in the header section of the webpage:")
    for link in header_links:
        print(link)
    
    print("All links found in the webpage:")
    for link in all_links:
        print(link)
    
    return list(all_links)

# Example usage:
starting_url = 'http://example.com'
collect_links = collect_links(starting_url)
